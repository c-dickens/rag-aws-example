{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Research Agent Demo\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) pipeline for healthcare document analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîß Setup\n",
    "\n",
    "First, let's import the necessary modules and load environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import our modules\n",
    "from tools import load_and_chunk_pdf\n",
    "from bedrock_wrapper import embed_texts, generate_answer\n",
    "from retriever import RAGRetriever\n",
    "from rag_pipeline import generate_answer_with_rag\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get S3 bucket name\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET_NAME\")\n",
    "if not S3_BUCKET:\n",
    "    raise ValueError(\"Please set S3_BUCKET_NAME environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available PDF Files\n",
    "\n",
    "Let's see what PDF files are available in our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available PDF files in bucket:\n",
      " - AdvaMed-AI-White-Paper-Final.pdf\n",
      " - isqua-white-paper-on-patient-safety-in-healthcare-organisations.pdf\n",
      " - mhs-iv-patient-safety-practices-year-2.pdf\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=S3_BUCKET)\n",
    "\n",
    "print(\"Available PDF files in bucket:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].lower().endswith('.pdf'):\n",
    "        print(f\" - {obj['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üì• Load & Embed Documents\n",
    "\n",
    "Let's load and process one of the PDFs to demonstrate the document processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AdvaMed-AI-White-Paper-Final.pdf...\n",
      "‚úÖ Created 66 chunks\n",
      "Example chunk:\n",
      "Source: AdvaMed-AI-White-Paper-Final.pdf\n",
      "Chunk ID: 0\n",
      "Text preview:      The Role of Artificial Intelligence (AI) in Healthcare \n",
      "\n",
      "Executive Summary \n",
      "\n",
      "Artificial intelligence (AI) applied to healthcare, driven by innovative medical technology, has and will \n",
      "continue to...\n"
     ]
    }
   ],
   "source": [
    "# Choose a PDF to process\n",
    "pdf_key = \"AdvaMed-AI-White-Paper-Final.pdf\"\n",
    "\n",
    "# Load and chunk the PDF\n",
    "print(f\"Processing {pdf_key}...\")\n",
    "chunks = load_and_chunk_pdf(S3_BUCKET, pdf_key)\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Display example chunk\n",
    "print(\"Example chunk:\")\n",
    "print(f\"Source: {chunks[0]['source']}\")\n",
    "print(f\"Chunk ID: {chunks[0]['chunk_id']}\")\n",
    "print(f\"Text preview: {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîç Run a Query (RAG)\n",
    "\n",
    "Now let's use the RAG pipeline to answer a question about healthcare AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index and chunks not found. Please run embed_and_store_chunks.py first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the key considerations for AI in medical devices according to the FDA?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get answer using RAG\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mgenerate_answer_with_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnswer:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/interview-projects/rag-research-agent/rag_pipeline.py:21\u001b[39m, in \u001b[36mgenerate_answer_with_rag\u001b[39m\u001b[34m(query, k)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mGenerate an answer using RAG (Retrieval-Augmented Generation).\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m    Dict[str, Any]: Dictionary containing the answer and sources\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Initialize retriever\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m retriever = \u001b[43mRAGRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Retrieve relevant chunks\u001b[39;00m\n\u001b[32m     24\u001b[39m chunks = retriever.retrieve(query, k)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/interview-projects/rag-research-agent/retriever.py:15\u001b[39m, in \u001b[36mRAGRetriever.__init__\u001b[39m\u001b[34m(self, index_path, chunks_path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.chunks = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_or_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/interview-projects/rag-research-agent/retriever.py:26\u001b[39m, in \u001b[36mRAGRetriever._load_or_initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded existing index with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.index.ntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vectors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex and chunks not found. Please run embed_and_store_chunks.py first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Index and chunks not found. Please run embed_and_store_chunks.py first."
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What are the key considerations for AI in medical devices according to the FDA?\"\n",
    "\n",
    "# Get answer using RAG\n",
    "result = generate_answer_with_rag(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in result[\"sources\"]:\n",
    "    print(f\" - {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Retrieved Chunks\n",
    "\n",
    "Let's look at the actual chunks that were retrieved to provide context for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index and chunks not found. Please run embed_and_store_chunks.py first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize retriever\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m retriever = \u001b[43mRAGRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Retrieve chunks\u001b[39;00m\n\u001b[32m      5\u001b[39m chunks = retriever.retrieve(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/interview-projects/rag-research-agent/retriever.py:15\u001b[39m, in \u001b[36mRAGRetriever.__init__\u001b[39m\u001b[34m(self, index_path, chunks_path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.chunks = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_or_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/interview-projects/rag-research-agent/retriever.py:26\u001b[39m, in \u001b[36mRAGRetriever._load_or_initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded existing index with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.index.ntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vectors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex and chunks not found. Please run embed_and_store_chunks.py first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Index and chunks not found. Please run embed_and_store_chunks.py first."
     ]
    }
   ],
   "source": [
    "# Initialize retriever\n",
    "retriever = RAGRetriever()\n",
    "\n",
    "# Retrieve chunks\n",
    "chunks = retriever.retrieve(query)\n",
    "\n",
    "print(\"Retrieved chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"Source: {chunk['source']}\")\n",
    "    print(f\"Text: {chunk['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ‚öñÔ∏è Compare RAG vs. No-RAG (baseline)\n",
    "\n",
    "Let's compare the RAG answer with a baseline answer that doesn't use retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 5) (3961657865.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "# Get baseline answer (no context)\n",
    "baseline_answer = generate_answer(query, [])\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"\n",
    "=== RAG Answer ===\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\n",
    "=== Baseline Answer (No Context) ===\")\n",
    "print(baseline_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚úÖ Wrap Up\n",
    "\n",
    "Let's try one more example query to demonstrate the system's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 8) (1703145086.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 8)\n"
     ]
    }
   ],
   "source": [
    "# Another example query\n",
    "query2 = \"What are the best practices for implementing AI in healthcare organizations?\"\n",
    "\n",
    "# Get answer using RAG\n",
    "result2 = generate_answer_with_rag(query2)\n",
    "\n",
    "print(\"Question:\", query2)\n",
    "print(\"\n",
    "Answer:\", result2[\"answer\"])\n",
    "print(\"\n",
    "Sources:\")\n",
    "for source in result2[\"sources\"]:\n",
    "    print(f\" - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
